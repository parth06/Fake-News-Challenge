{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bidirectional LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parth06/Fake-News-Challenge/blob/master/Neural%20Net/Bidirectional_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyDWwYIb7Fao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/parth06/Fake-News-Challenge.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQdkO7I-7QG0",
        "colab_type": "code",
        "outputId": "a6503330-8f0f-4532-a9b8-22059e5cc522",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd Fake-News-Challenge/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Fake-News-Challenge\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kljoX4nZ7Rmn",
        "colab_type": "code",
        "outputId": "e109e777-556d-4c00-a849-b156f0acf061",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!git submodule init\n",
        "!git submodule update"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Submodule 'fnc-1' (https://github.com/FakeNewsChallenge/fnc-1) registered for path 'fnc-1'\n",
            "Cloning into '/content/Fake-News-Challenge/fnc-1'...\n",
            "Submodule path 'fnc-1': checked out '29d473af2d15278f0464d5e41e4cbe7eb58231f2'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WGbUidG7TIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XfvJlJo7UzJ",
        "colab_type": "code",
        "outputId": "4c594809-de90-45a3-a5a7-0347c849a82f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zloyYD0g7WY0",
        "colab_type": "code",
        "outputId": "8f1b7844-654a-42f9-eca7-28ac901c8b10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "!mkdir data/glove\n",
        "!mkdir data/glove_twitter\n",
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        "!unzip glove.twitter.27B.zip -d data/glove_twitter/\n",
        "!unzip glove.6B.zip -d data/glove"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-26 11:17:52--  http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.twitter.27B.zip [following]\n",
            "--2019-07-26 11:17:52--  https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip [following]\n",
            "--2019-07-26 11:17:53--  http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520408563 (1.4G) [application/zip]\n",
            "Saving to: ‘glove.twitter.27B.zip’\n",
            "\n",
            "glove.twitter.27B.z 100%[===================>]   1.42G  19.1MB/s    in 91s     \n",
            "\n",
            "2019-07-26 11:19:24 (15.9 MB/s) - ‘glove.twitter.27B.zip’ saved [1520408563/1520408563]\n",
            "\n",
            "--2019-07-26 11:19:25--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-07-26 11:19:26--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2019-07-26 11:19:26--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  18.5MB/s    in 47s     \n",
            "\n",
            "2019-07-26 11:20:14 (17.4 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.twitter.27B.zip\n",
            "  inflating: data/glove_twitter/glove.twitter.27B.25d.txt  \n",
            "  inflating: data/glove_twitter/glove.twitter.27B.50d.txt  \n",
            "  inflating: data/glove_twitter/glove.twitter.27B.100d.txt  \n",
            "  inflating: data/glove_twitter/glove.twitter.27B.200d.txt  \n",
            "Archive:  glove.6B.zip\n",
            "  inflating: data/glove/glove.6B.50d.txt  \n",
            "  inflating: data/glove/glove.6B.100d.txt  \n",
            "  inflating: data/glove/glove.6B.200d.txt  \n",
            "  inflating: data/glove/glove.6B.300d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRym1Qum7dfU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm glove.twitter.27B.zip\n",
        "!rm glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxaCoDtS7lhZ",
        "colab_type": "code",
        "outputId": "1f9b2a56-7c57-4e49-cc88-c3b403f8f74f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import codecs\n",
        "import sys\n",
        "import pickle\n",
        "\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras import callbacks\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, BatchNormalization, Activation, Bidirectional\n",
        "from keras.layers.merge import concatenate,subtract\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import matplotlib as mpl\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.utils import plot_model \n",
        "from IPython.display import Image\n",
        "import pydot\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "from string import punctuation\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "#from keras import initializations\n",
        "from keras import initializers, regularizers, constraints\n",
        "\n",
        "tf_gpu_options = tf.GPUOptions(allow_growth = True) # per_process_gpu_memory_fraction=0.12,\n",
        "tf_session = tf.Session(config=tf.ConfigProto(gpu_options=tf_gpu_options))\n",
        "tf.keras.backend.set_session(tf_session)\n",
        "\n",
        "#biodirectional embedding\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO381QcY7pqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify the folder locations\n",
        "#W2V_DIR = './data/GoogleNews-vectors-negative300.bin' #\n",
        "GloVe_DIR = './data/glove_twitter/glove.twitter.27B.50d.txt'\n",
        "#the data directory\n",
        "DATA_DIR = './data/old'\n",
        "# These are some hyperparameters that can be tuned\n",
        "MAX_SENT_LEN = 300 #75(0.68), 150, 300 700(90% but too time comsuming)\n",
        "MAX_VOCAB_SIZE = 28000 #vocabulary\n",
        "LSTM_DIM = 50#len(embd[0])\n",
        "EMBEDDING_DIM = 50 #50 for GloVe 300 for w2v\n",
        "BATCH_SIZE = 128\n",
        "N_EPOCHS = 40 #40"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WXTHc2j70vh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fix random seed for reproducibility\n",
        "seed = 1\n",
        "np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2BgZd3d73CW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the text files of fnc data\n",
        "bodies = pd.read_csv(DATA_DIR + '/body_id.csv')\n",
        "train_df = pd.read_csv(DATA_DIR + '/train.csv')\n",
        "validation_df = pd.read_csv(DATA_DIR + '/validation.csv')\n",
        "test_df = pd.read_csv(DATA_DIR + '/test.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UB0-hyL779D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.replace('unrelated',1,True)\n",
        "train_df.replace('agree',2,True)\n",
        "train_df.replace('disagree',3,True)\n",
        "train_df.replace('discuss',4,True)\n",
        "\n",
        "validation_df.replace('unrelated',1,True)\n",
        "validation_df.replace('agree',2,True)\n",
        "validation_df.replace('disagree',3,True)\n",
        "validation_df.replace('discuss',4,True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPqWJdxirmPn",
        "colab_type": "code",
        "outputId": "96b8a1af-d3ae-4915-ba09-77258290382b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(bodies[\"articleBody\"][0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1902"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FUm-l7j7_Um",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Unrelated:1 Related:0\n",
        "def f(row):\n",
        "    if row['Stance']==4:\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "train_df['Classify'] = train_df.apply(f, axis=1)\n",
        "validation_df['Classify'] = validation_df.apply(f, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJiLTtg68BeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combine_df_train = train_df.join(bodies.set_index('Body ID'), on='Body ID')\n",
        "combine_df_validation = validation_df.join(bodies.set_index('Body ID'), on='Body ID')\n",
        "combine_df_test = test_df.join(bodies.set_index('Body ID'), on='Body ID')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6LpcxJ48Ely",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pre-processing involves removal of puctuations and converting text to lower case\n",
        "word_seq_head_train = [text_to_word_sequence(head) for head in combine_df_train['Headline']]\n",
        "word_seq_bodies_train = [text_to_word_sequence(body) for body in combine_df_train['articleBody']]\n",
        "word_seq_head_validation = [text_to_word_sequence(head) for head in combine_df_validation['Headline']]\n",
        "word_seq_bodies_validation = [text_to_word_sequence(body) for body in combine_df_validation['articleBody']]\n",
        "word_seq_head_test = [text_to_word_sequence(head) for head in combine_df_test['Headline']]\n",
        "word_seq_bodies_test = [text_to_word_sequence(body) for body in combine_df_test['articleBody']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bLu1Gta8GbU",
        "colab_type": "code",
        "outputId": "d3819d24-e979-43da-93fa-74f95999e9f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('90th Percentile Sentence of headline:', np.percentile([len(seq) for seq in word_seq_head_train], 90))\n",
        "print('90th Percentile Sentence of body:', np.percentile([len(seq) for seq in word_seq_bodies_train], 50))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "90th Percentile Sentence of headline: 16.0\n",
            "90th Percentile Sentence of body: 308.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f0DoTVR8Hu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_seq = []\n",
        "for i in range(len(word_seq_head_train)):\n",
        "    word_seq.append(word_seq_head_train[i])\n",
        "for i in range(len(word_seq_bodies_train)):\n",
        "    word_seq.append(word_seq_bodies_train[i])\n",
        "\n",
        "# for i in range(len(word_seq_head_validation)):\n",
        "#     word_seq.append(word_seq_head_validation[i])\n",
        "# for i in range(len(word_seq_bodies_validation)):\n",
        "#     word_seq.append(word_seq_bodies_validation[i])\n",
        "    \n",
        "# for i in range(len(word_seq_head_test)):\n",
        "#     word_seq.append(word_seq_head_test[i])\n",
        "# for i in range(len(word_seq_bodies_test)):\n",
        "#     word_seq.append(word_seq_bodies_test[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KevM-0W48SHK",
        "colab_type": "code",
        "outputId": "11769e84-534d-46b6-b5af-a4d92042e022",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "filter_list = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "MAX_VOCAB_SIZE = 25000\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters=filter_list)\n",
        "tokenizer.fit_on_texts([seq for seq in word_seq])\n",
        "#because it only includes unique words(tokens)\n",
        "\n",
        "print(\"Number of words in vocabulary:\", len(tokenizer.word_index))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in vocabulary: 25128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIdPx5bo9USt",
        "colab_type": "code",
        "outputId": "93d25fc3-999b-4660-cf7a-bd69d1be749f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Load the word2vec embeddings \n",
        "#embeddings = gensim.models.KeyedVectors.load_word2vec_format(GloVe_DIR, binary=True)\n",
        "\n",
        "#GloVes Load\n",
        "glove_input_file = GloVe_DIR\n",
        "word2vec_output_file = 'glove.50d.txt.word2vec'\n",
        "glove2word2vec(glove_input_file, word2vec_output_file)\n",
        "embeddings = gensim.models.KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHuo9WeX8pwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(tokenizer.word_index)+1, EMBEDDING_DIM)) # +1 is because the matrix indices start with 0\n",
        "\n",
        "for word, i in tokenizer.word_index.items(): # i=0 is the embedding for the zero padding\n",
        "  try:\n",
        "      embeddings_vector = embeddings[word]\n",
        "  except KeyError:\n",
        "      embeddings_vector = None\n",
        "  if embeddings_vector is not None:\n",
        "      embeddings_matrix[i] = embeddings_vector\n",
        "\n",
        "del embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1Xc9FfC832u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_HEAD_LEN = 15\n",
        "MAX_BODY_LEN = 300\n",
        "# Shorten the sentence to a fixed length\n",
        "# Convert the sequence of words to sequnce of indices\n",
        "X_head_train = tokenizer.texts_to_sequences([' '.join(seq[:MAX_HEAD_LEN]) for seq in word_seq_head_train])\n",
        "X_head_train = pad_sequences(X_head_train, maxlen=MAX_HEAD_LEN, padding='post', truncating='post')\n",
        "\n",
        "X_body_train = tokenizer.texts_to_sequences([' '.join(seq[:MAX_BODY_LEN]) for seq in word_seq_bodies_train])\n",
        "X_body_train = pad_sequences(X_body_train, maxlen=MAX_BODY_LEN, padding='post', truncating='post')\n",
        "\n",
        "y_stance_train = combine_df_train['Stance']\n",
        "y_classify_train = combine_df_train['Classify']\n",
        "\n",
        "\n",
        "#Validation\n",
        "X_head_val = tokenizer.texts_to_sequences([' '.join(seq[:MAX_HEAD_LEN]) for seq in word_seq_head_validation])\n",
        "X_head_val = pad_sequences(X_head_val, maxlen=MAX_HEAD_LEN, padding='post', truncating='post')\n",
        "\n",
        "X_body_val = tokenizer.texts_to_sequences([' '.join(seq[:MAX_BODY_LEN]) for seq in word_seq_bodies_validation])\n",
        "X_body_val = pad_sequences(X_body_val, maxlen=MAX_BODY_LEN, padding='post', truncating='post')\n",
        "\n",
        "y_stance_val = combine_df_validation['Stance']\n",
        "y_classify_val = combine_df_validation['Classify']\n",
        "\n",
        "#TEST\n",
        "# Convert the sequence of words to sequnce of indices\n",
        "X_head_test = tokenizer.texts_to_sequences([' '.join(seq[:MAX_HEAD_LEN]) for seq in word_seq_head_test])\n",
        "X_head_test = pad_sequences(X_head_test, maxlen=MAX_BODY_LEN, padding='post', truncating='post')\n",
        "\n",
        "X_body_test = tokenizer.texts_to_sequences([' '.join(seq[:MAX_BODY_LEN]) for seq in word_seq_bodies_test])\n",
        "X_body_test = pad_sequences(X_body_test, maxlen=MAX_BODY_LEN, padding='post', truncating='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMNijFXj-b3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Encode class values as integers\n",
        "encoder_train = LabelEncoder()\n",
        "encoder_train.fit(y_stance_train)\n",
        "encoded_train = encoder_train.transform(y_stance_train)\n",
        "encoded_val = encoder_train.transform(y_stance_val)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "dummy_ys_train = np_utils.to_categorical(encoded_train)\n",
        "dummy_ys_val = np_utils.to_categorical(encoded_val)\n",
        "\n",
        "#Classify encoding\n",
        "encoder_train = LabelEncoder()\n",
        "encoder_train.fit(y_classify_train)\n",
        "encoded_train = encoder_train.transform(y_classify_train)\n",
        "encoded_val = encoder_train.transform(y_classify_val)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "dummy_yc_train = np_utils.to_categorical(encoded_train)\n",
        "dummy_yc_val = np_utils.to_categorical(encoded_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4_XRcEoAz1e",
        "colab_type": "code",
        "outputId": "99404ef6-f0d6-411e-a131-779b96dab478",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(X_body_train[1])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAlC3382Go4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import add"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXjG0cMNAYuc",
        "colab_type": "code",
        "outputId": "594d90cf-5cc2-44c6-ca00-875e06368d77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "source": [
        "# Build a sequential model by stacking neural net units \n",
        "left = Sequential()\n",
        "left.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
        "                          output_dim=EMBEDDING_DIM,\n",
        "                          weights = [embeddings_matrix], trainable=False, name='head_word_embedding_layer', #False\n",
        "                          mask_zero=True))\n",
        "left.add(Bidirectional(LSTM(LSTM_DIM, return_sequences=False, name='head_Bidrectional_lstm_layer1')))\n",
        "\n",
        "right = Sequential()\n",
        "right.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
        "                          output_dim=EMBEDDING_DIM,\n",
        "                          weights = [embeddings_matrix], trainable=False, name='body_word_embedding_layer', #False\n",
        "                          mask_zero=True))\n",
        "right.add(Bidirectional(LSTM(LSTM_DIM, return_sequences=False, name='body_Bidrectional_lstm_layer2')))\n",
        "\n",
        "\n",
        "concatenated = concatenate([left.output,right.output])\n",
        "layer_1 = Dense(100, activation='relu', name='layer_1') (concatenated)\n",
        "dropout_1 = Dropout(rate=0.5, name='dropout_1')(layer_1)\n",
        "layer_2 =Dense(50,  activation='relu', name='layer_2') (dropout_1)\n",
        "dropout_2 = Dropout(rate=0.5, name='dropout_2')(layer_2)\n",
        "# layer_3 =Dense(600,  activation='relu', name='layer_3') (dropout_2)\n",
        "# dropout_3 = Dropout(rate=0.5, name='dropout_3')(layer_3)\n",
        "# out1 =  Dense(2, activation='softmax', name='output_layer1')(dropout_3)\n",
        "out2 = Dense(4, activation='softmax', name='output_layer2')(dropout_2)\n",
        "\n",
        "model = Model([left.input, right.input], out2)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0726 11:28:25.067631 140354626779008 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0726 11:28:25.071304 140354626779008 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0726 11:28:25.084535 140354626779008 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0726 11:28:25.098416 140354626779008 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0726 11:28:25.099775 140354626779008 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0726 11:28:27.255477 140354626779008 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0726 11:28:28.171996 140354626779008 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "head_word_embedding_layer_input (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "body_word_embedding_layer_input (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "head_word_embedding_layer (Embe (None, None, 50)     1256450     head_word_embedding_layer_input[0\n",
            "__________________________________________________________________________________________________\n",
            "body_word_embedding_layer (Embe (None, None, 50)     1256450     body_word_embedding_layer_input[0\n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 100)          40400       head_word_embedding_layer[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 100)          40400       body_word_embedding_layer[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 200)          0           bidirectional_1[0][0]            \n",
            "                                                                 bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "layer_1 (Dense)                 (None, 100)          20100       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 100)          0           layer_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_2 (Dense)                 (None, 50)           5050        dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 50)           0           layer_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "output_layer2 (Dense)           (None, 4)            204         dropout_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 2,619,054\n",
            "Trainable params: 106,154\n",
            "Non-trainable params: 2,512,900\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAnThMX8HJao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model, show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfGKNzDRJDSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWEeaCMiI6D-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filepath=\"./result/lstmBidirectional_lstm_50token_lr0.001_trainable_{epoch:02d}_{val_acc:.4f}.h5\"\n",
        "# checkpoint = callbacks.ModelCheckpoint(filepath, \n",
        "#                                        monitor='val_acc', \n",
        "#                                        verbose=0, \n",
        "#                                        save_best_only=True)\n",
        "es = callbacks.EarlyStopping(monitor='val_acc', mode='max',patience=2)\n",
        "callbacks_list1 = [es]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McNZvi4AJM3-",
        "colab_type": "code",
        "outputId": "a8e6e32b-b144-4304-eee6-01c6f356bd91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0726 11:28:30.678092 140354626779008 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0jhv-8LIhoX",
        "colab_type": "code",
        "outputId": "863476c3-ae6e-4b85-e77c-0c7bf2afdde6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "history = model.fit([X_head_train,X_body_train], y = dummy_ys_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=N_EPOCHS,\n",
        "                    verbose=1,\n",
        "          validation_data=([X_head_val,X_body_val], dummy_ys_val),callbacks = callbacks_list1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40350 samples, validate on 9622 samples\n",
            "Epoch 1/40\n",
            "40350/40350 [==============================] - 574s 14ms/step - loss: 0.8193 - acc: 0.7226 - val_loss: 0.6861 - val_acc: 0.7333\n",
            "Epoch 2/40\n",
            "40350/40350 [==============================] - 579s 14ms/step - loss: 0.6127 - acc: 0.7659 - val_loss: 0.6258 - val_acc: 0.7552\n",
            "Epoch 3/40\n",
            "40350/40350 [==============================] - 694s 17ms/step - loss: 0.4945 - acc: 0.8073 - val_loss: 0.5937 - val_acc: 0.7571\n",
            "Epoch 4/40\n",
            "40350/40350 [==============================] - 551s 14ms/step - loss: 0.4071 - acc: 0.8445 - val_loss: 0.5452 - val_acc: 0.7823\n",
            "Epoch 5/40\n",
            "40350/40350 [==============================] - 552s 14ms/step - loss: 0.3447 - acc: 0.8687 - val_loss: 0.5359 - val_acc: 0.8066\n",
            "Epoch 6/40\n",
            "40350/40350 [==============================] - 550s 14ms/step - loss: 0.2901 - acc: 0.8920 - val_loss: 0.5514 - val_acc: 0.8001\n",
            "Epoch 7/40\n",
            " 7552/40350 [====>.........................] - ETA: 6:57 - loss: 0.2640 - acc: 0.9020"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqtPPT2aJvVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.evaluate([X_head_train,X_body_train],y=dummy_ys_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr9BC1F-Oipk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yt = model.predict([X_head_test,X_body_test])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAXt-rkgOeei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yt = np.argmax(yt,axis=1)\n",
        "ans = pd.DataFrame(yt)\n",
        "ans.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9Gx-xXnMlvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans.replace(0,'unrelated',True)\n",
        "ans.replace(1,'agree',True)\n",
        "ans.replace(2,'disagree',True)\n",
        "ans.replace(3,'discuss',True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnaPYwLMMxNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from feature_engineering import refuting_features, polarity_features, hand_features, gen_or_load_feats\n",
        "from feature_engineering import word_overlap_features\n",
        "from utils.dataset import DataSet\n",
        "from utils.generate_test_splits import kfold_split, get_stances_for_folds\n",
        "from utils.score import report_score, LABELS, score_submission\n",
        "\n",
        "from utils.system import parse_params, check_version\n",
        "def generate_features(stances,dataset,name):\n",
        "    h, b, y = [],[],[]\n",
        "\n",
        "    for stance in stances:\n",
        "        y.append(LABELS.index(stance['Stance']))\n",
        "        h.append(stance['Headline'])\n",
        "        b.append(dataset.articles[stance['Body ID']])\n",
        "\n",
        "    return y\n",
        "  \n",
        "\n",
        "competition_dataset = DataSet(\"competition_test\")\n",
        "y_competition = generate_features(competition_dataset.stances, competition_dataset, \"competition\")\n",
        "\n",
        "Xs = dict()\n",
        "ys = dict()\n",
        "\n",
        "# Load/Precompute all features now\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgxJ2DbMM4ZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Run on competition dataset\n",
        "actual = [LABELS[int(a)] for a in y_competition]\n",
        "\n",
        "print(\"Scores on the test set\")\n",
        "report_score(actual,ans[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QXYj9hTMxQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy(Truncation: 150 Epoch: 10)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}